{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "# categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3387 documents\n",
      "4 categories\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2839)\t0.04500071507017083\n",
      "  (0, 1414)\t0.07540974760886272\n",
      "  (0, 8358)\t0.04976131971209157\n",
      "  (0, 1108)\t0.0841691125267977\n",
      "  (0, 4745)\t0.0841691125267977\n",
      "  (0, 2670)\t0.08189317880141146\n",
      "  (0, 2244)\t0.07995160596271345\n",
      "  (0, 6946)\t0.05452908892155513\n",
      "  (0, 9698)\t0.06016650695202247\n",
      "  (0, 3540)\t0.08913144120477544\n",
      "  (0, 2324)\t0.05804798267620172\n",
      "  (0, 9211)\t0.043848362254850295\n",
      "  (0, 3401)\t0.054009661453737326\n",
      "  (0, 3001)\t0.028323774603401228\n",
      "  (0, 6174)\t0.06071840775718862\n",
      "  (0, 2190)\t0.0903937008980028\n",
      "  (0, 8443)\t0.07047414689713123\n",
      "  (0, 6960)\t0.060439582663268904\n",
      "  (0, 6175)\t0.06805953438858334\n",
      "  (0, 2829)\t0.08913144120477544\n",
      "  (0, 7709)\t0.0805677032230983\n",
      "  (0, 1084)\t0.05302819749691229\n",
      "  (0, 1760)\t0.05851683527589977\n",
      "  (0, 9881)\t0.03326725261069873\n",
      "  (0, 7923)\t0.04852708902991879\n",
      "  :\t:\n",
      "  (3386, 6705)\t0.04636574358198852\n",
      "  (3386, 208)\t0.073055490407179\n",
      "  (3386, 8947)\t0.058413220065501995\n",
      "  (3386, 6133)\t0.05509573409226583\n",
      "  (3386, 5417)\t0.12261715332194234\n",
      "  (3386, 4683)\t0.15084827614247748\n",
      "  (3386, 2165)\t0.11482161605032963\n",
      "  (3386, 5087)\t0.06381270757122054\n",
      "  (3386, 5395)\t0.15310799606496223\n",
      "  (3386, 5456)\t0.06423374636793071\n",
      "  (3386, 8412)\t0.08300888851372776\n",
      "  (3386, 304)\t0.08350610582125675\n",
      "  (3386, 6474)\t0.07315255477001704\n",
      "  (3386, 6241)\t0.17522991214494527\n",
      "  (3386, 4685)\t0.12498320499549762\n",
      "  (3386, 6954)\t0.06049227697380511\n",
      "  (3386, 2112)\t0.1670122116425135\n",
      "  (3386, 8331)\t0.2488647336670735\n",
      "  (3386, 4104)\t0.24692113531279744\n",
      "  (3386, 4424)\t0.034286035217165646\n",
      "  (3386, 6958)\t0.033212553876237796\n",
      "  (3386, 6218)\t0.03451329591620291\n",
      "  (3386, 8873)\t0.06669826131063836\n",
      "  (3386, 9369)\t0.07501265631943817\n",
      "  (3386, 6437)\t0.07488347353395008\n",
      "done in 0.944644s\n",
      "n_samples: 3387, n_features: 10000\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=True)\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "print(X)\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction using LSA\n",
      "done in 0.116989s\n",
      "Explained variance of the SVD step: 5%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing dimensionality reduction using LSA\")\n",
    "t0 = time()\n",
    "# Vectorizer results are normalized, which makes KMeans behave as\n",
    "# spherical k-means for better results. Since LSA/SVD results are\n",
    "# not normalized, we have to redo the normalization.\n",
    "svd = TruncatedSVD(10)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X = lsa.fit_transform(X)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(\n",
    "    int(explained_variance * 100)))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with KMeans(max_iter=100, n_clusters=4, n_init=1, verbose=False)\n",
      "done in 0.016s\n",
      "\n",
      "Homogeneity: 0.608\n",
      "Completeness: 0.618\n",
      "V-measure: 0.613\n",
      "Adjusted Rand-Index: 0.632\n",
      "Silhouette Coefficient: 0.313\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0: graphics image com file files university ac thanks images 3d\n",
      "Cluster 1: god jesus people com don bible believe just think say\n",
      "Cluster 2: space nasa gov alaska access shuttle com digex henry just\n",
      "Cluster 3: com sandvik keith morality sgi objective caltech livesey kent moral\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=False)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "if True:\n",
    "    original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "    order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "else:\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
